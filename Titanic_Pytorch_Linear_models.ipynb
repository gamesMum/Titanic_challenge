{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Titanic with Pytorch\nHello there! We'll try to solve the Titanic survival model first as a Linear Regression problem and second as classification problem.\n\n- We'll take a look at our data.\n- Choose the features (Featur selection)\n- Creating our custom dataset and batches using Pytorch DataLoader which will make it an easy task.\n\nLet's dive in!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import all the necessary libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#This is for data exploration and viualization\nimport seaborn as sns \nfrom scipy import stats\nimport matplotlib.pyplot as plt \n\n# The following is basically for building and training our models\nimport torch \nfrom torch import nn \nimport torch.optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/titanic/train.csv')\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/titanic/test.csv')\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the length and columns\nlen(train_data.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_data.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find survival rate for women\nwomen = train_data.loc[train_data.Sex=='female'][\"Survived\"]\nrate_women = sum(women) / len(women)\nF\"% of women who survived: {rate_women}\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"men = train_data.loc[train_data.Sex=='male']['Survived']\nmen_rate = sum(men) / len(men)\nF\"% of men who survived: {men_rate}\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the survival rate in the different classes\nfare_mean_1st = train_data[train_data[\"Pclass\"]==1].Fare.mean()\nfare_mean_2nd = train_data[train_data[\"Pclass\"]==2].Fare.mean()\nfare_mean_3rd = train_data[train_data[\"Pclass\"]==3].Fare.mean()\nF\"Average cost of tickets for 1st, snd, 3rd classes: \\\n{fare_mean_1st} || {fare_mean_2nd} || {fare_mean_3rd}\"\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's see how were the effect of other factors in the rate of survival.\nThe code below might not be the easiest to read, but if we take a second good look it will be clear tp us that we are dividing the numbers of survivals (men/women) by the number of passangers (survived or not) in the specific class.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"woman_survived_1st = len(train_data[(train_data[\"Sex\"]==\"female\") & (train_data[\"Survived\"]==1) & (train_data[\"Pclass\"]==1)].index) / len(train_data[(train_data[\"Sex\"]==\"female\") & (train_data[\"Pclass\"]==1)].index)\nwoman_survived_2nd = len(train_data[(train_data[\"Sex\"]==\"female\") & (train_data[\"Survived\"]==1) & (train_data[\"Pclass\"]==2)].index) / len(train_data[(train_data[\"Sex\"]==\"female\") & (train_data[\"Pclass\"]==2)].index)\nwoman_survived_3rd = len(train_data[(train_data[\"Sex\"]==\"female\") & (train_data[\"Survived\"]==1) & (train_data[\"Pclass\"]==3)].index) / len(train_data[(train_data[\"Sex\"]==\"female\") & (train_data[\"Pclass\"]==3)].index)\n\nF\"Rate of Survival for women in different classes: {woman_survived_1st} || {woman_survived_2nd} || {woman_survived_3rd}\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's find out what other factors could effect the rate of survival\nwoman_survived_1st = len(train_data[(train_data[\"Sex\"]==\"male\") & (train_data[\"Survived\"]==1) & (train_data[\"Pclass\"]==1)].index) / len(train_data[(train_data[\"Sex\"]==\"male\") & (train_data[\"Pclass\"]==1)].index)\nwoman_survived_2nd = len(train_data[(train_data[\"Sex\"]==\"male\") & (train_data[\"Survived\"]==1) & (train_data[\"Pclass\"]==2)].index) / len(train_data[(train_data[\"Sex\"]==\"male\") & (train_data[\"Pclass\"]==2)].index)\nwoman_survived_3rd = len(train_data[(train_data[\"Sex\"]==\"male\") & (train_data[\"Survived\"]==1) & (train_data[\"Pclass\"]==3)].index) / len(train_data[(train_data[\"Sex\"]==\"male\") & (train_data[\"Pclass\"]==3)].index)\n\nF\"Rate of Survival for men in different classes: {woman_survived_1st} || {woman_survived_2nd} || {woman_survived_3rd}\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The verage Age \nage_mean = train_data.Age.mean()\nsurvived_age_mean = train_data[(train_data[\"Survived\"]==1)].Age.mean()\nsurvived_age_std = train_data[(train_data[\"Survived\"]==1)].Age.std()\nsurvived_min_age = train_data[(train_data[\"Survived\"]==1)].Age.min()\nsurvived_max_age = train_data[(train_data[\"Survived\"]==1)].Age.max()\n\nprint(\"The average of survivals age \", survived_age_mean)\nprint(\"The STD of survivals age \", survived_age_std)\nprint(\"The min age of survivals \", survived_min_age)\nprint(\"The max age of survivals \", survived_max_age)\n\n# Let's see the other side\ndeceased_age_mean = train_data[(train_data[\"Survived\"]==0)].Age.mean()\ndeceased_age_std = train_data[(train_data[\"Survived\"]==0)].Age.std()\ndeceased_min_age = train_data[(train_data[\"Survived\"]==0)].Age.min()\ndeceased_max_age = train_data[(train_data[\"Survived\"]==0)].Age.max()\n\nprint()\nprint(\"The average of deceased age\", deceased_age_mean)\nprint(\"The STD of deceased age\", deceased_age_std)\nprint(\"The min age of deceased \", deceased_min_age)\nprint(\"The max age of deceased \", deceased_max_age)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Investigating Deeper into the data\nI actually added this part after unsuccessfully triec to increase the accuracy for my model both linear regression and classification. So I decided to explore the data more and see if there is any Null values or/and outliers that could effect the model results.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show Any Null values in the data\ntrain_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Box Plot is a ggod way to detect outlier values. They are the invisual dots away from the quartiles.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=train_data['Age'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=train_data['SibSp'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just to have a better visualization for our data let's have a better look with some histograms","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig= plt.figure(figsize=(10,5))\ntrain_data.groupby('Sex')['PassengerId'].nunique().plot(kind='bar')\nplt.xlabel('Sex')\nplt.title('Number of records by Sex')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig= plt.figure(figsize=(20,10))\ntrain_data.groupby('Age')['PassengerId'].nunique().plot(kind='bar')\nplt.xlabel('Age')\nplt.title('Number of records by Age')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Before ficing our data let's drop any undesired columns\nX = train_data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked'], axis=1)\nX_test = test_data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing Data\nBut first make sure that we convert categorical data into one hot encoding and fix any NAN values in the train data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.get_dummies(X)\nX_test = pd.get_dummies(X_test)\n\nX.fillna(X.mean(),inplace=True)\nX_test.fillna(X_test.mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Awesome! No more null values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting data into labels and targets\nfeatures = [\"Pclass\", \"Sex_female\", \"Age\", \"Fare\", \"SibSp\", \"Parch\"]\n\n\n# Dividing the data into features and labels\ny= X['Survived']\n\nX = pd.DataFrame(X, columns = features) \nX_test = pd.DataFrame(X_test, columns = features)\n\n# standardize and Normalizing the data\n# This will also help minimize the effect of outliers \nfor col in features:\n    X[col] = (X[col] - X[col].mean()) / X[col].std()\n    X_test[col] = (X_test[col] - X_test[col].mean()) / X_test[col].std()\n    \nfor col in features:    \n    X[col] = (X[col] - X[col].min()) / (X[col].max() - X[col].min())\n    X_test[col] = (X_test[col] - X_test[col].min()) / (X_test[col].max() - X_test[col].min())\n       ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training the Model with Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=0)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('random_forest_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"random_forest_submission.csv\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert to numpy array \nX = X.to_numpy()\ny = y.to_numpy().reshape(-1, 1)\nX_test = X_test.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"F\"Length of train data: {len(X)}  Length of test data: {len(X_test)} \"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Chaeck the types of the data and shapes\nprint(\"The type of our data:\\n\", type(X))\nprint(type(X_test))\n\n\n\n# Print the shapes\nprint(\"\\nThe shape of our training data: \\n\", X.shape)\nprint(\"\\nThe shape of our targets: \\n\", y.shape)\nprint(\"\\nThe shape of our test data: \\n\", X_test.shape)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Batching the Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Batch the data for the linear regression\ndef batch_data(batch_size, input_data, target, test_data, train_type = \"regression\", val_size=0.1):\n    '''\n    This function batches the data for our model to train on\n    batch_size: number of batches to perform backpropagation on\n    input_data: numpy array with our input features\n    target: numpy array with our target\n    test_data: numpy array of our test data (doesn't contain targets)\n    train_type: some small differences in batches between regression vs classification\n    '''\n    if train_type == \"regression\":\n         target_tensor = torch.FloatTensor(target)\n            \n    elif train_type == \"classification\":\n        target_tensor = torch.LongTensor(target)\n        target_tensor = target_tensor.squeeze()\n        \n    input_tensor = torch.FloatTensor(input_data)\n    test_tensor = torch.FloatTensor(test_data)\n    \n     # Create our custom dataset with input and corresponding targets\n    train_dataset = TensorDataset(input_tensor, target_tensor)\n    \n    # Split training set into validation and training\n    num_train = len(train_dataset)\n    indicies = list(range(num_train))\n    np.random.shuffle(indicies)\n    val_split = int(np.floor(val_size * num_train))\n    \n    train_idx, val_idx = indicies[val_split:], indicies[:val_split]\n    \n    train_sampler = SubsetRandomSampler(train_idx)\n    val_sampler = SubsetRandomSampler(val_idx)\n    \n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler = train_sampler)\n    val_loader = DataLoader(train_dataset, batch_size=batch_size, sampler = val_sampler)\n    # No need to create multiple batches for the test loader\n    test_loader = DataLoader(test_tensor, batch_size=len(test_data))\n    \n    return train_loader, val_loader, test_loader\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 64\ntrain_loader, val_loader, test_loader = batch_data(batch_size, X, y, X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check our data loader\ndata_iter = iter(train_loader)\nsample_x, sample_y = data_iter.next()\n\nprint(sample_x.shape)\nprint(sample_x)\nprint()\nprint(sample_y.shape)\nprint(sample_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building the Linear Regression Model\n\nNow that everything is looking good, let's build our training model!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass LinearRegression(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(6, 1)     \n    def forward(self, x):\n        x = self.fc1(x)\n        return x\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_regression_model = LinearRegression()\nprint(linear_regression_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the loss and opimization functions\nlr = 0.001\ncriterion = nn.MSELoss() # mean square error\noptimizer = torch.optim.SGD(linear_regression_model.parameters(), lr=lr, momentum=0.9)\nbatch_size = 64","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training the Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, batch_size, epochs, cost_function, print_every = 100):\n    val_loss_min = np.Inf\n    \n    for e in range(epochs):\n        val_loss = 0.0\n        train_loss = 0.0\n        \n        model.train()\n        for inputs, labels in train_loader:\n        \n            optimizer.zero_grad()\n            output = model(inputs)\n            loss = cost_function(output, labels)\n            \n             # Perform the backpropagation and the optimization step\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item() * batch_size\n        \n        # Evaluating our model performance\n        model.eval()\n        for inputs, labels in val_loader:\n            output = model(inputs)\n            loss = cost_function(output, labels)\n            \n            val_loss += loss.item() * batch_size\n                    \n        train_loss = train_loss / len(train_loader.sampler)\n        val_loss = val_loss / len(val_loader.sampler)\n        \n        # save model if validation loss has decreased\n        if val_loss <= val_loss_min:\n            # print the decremnet in the validation\n            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n            val_loss_min, val_loss))\n            val_loss_min = val_loss\n            torch.save(model.state_dict(), 'model_linear.pt')\n            \n        if epochs % print_every == 0:\n            print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(e, train_loss, val_loss))\n        \n    print(\"Best model with validation loss: {}\". format(val_loss_min))\n\n \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model(linear_regression_model, batch_size, epochs=3000, cost_function=criterion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_regression_model.load_state_dict(torch.load('model_linear.pt'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting a batch from testing data\nwith torch.no_grad():\n    for data in test_loader:\n        output = linear_regression_model(data)\n        preds = torch.round(output)\n    preds = preds.squeeze()\n    survived = preds.numpy()\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"survived = survived.astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived': survived})\nsubmission.to_csv('submission_regression.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"submission_regression.csv\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Building the Classification  Model\n- The number of output here will change into 2\n- We'll use CrossEntropyLoss instead of MSELoss","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader, val_loader, test_loader = batch_data(batch_size, X, y, X_test, train_type=\"classification\", val_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass Clasification(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(6, 512)\n        self.fc2 = nn.Linear(512, 128)\n        self.fc3 = nn.Linear(128, 2)\n        self.dropout = nn.Dropout(0.5)\n        \n    def forward(self, x):\n        x = self.dropout(F.relu(self.fc1(x)))\n        x = self.dropout(F.relu(self.fc2(x)))\n        x = self.fc3(x)\n        return x\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classification_model = Clasification()\nclassification_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the loss and opimization functions\nlr = 0.0005\ncriterion = nn.CrossEntropyLoss() # mean square error\noptimizer = torch.optim.SGD(classification_model.parameters(), lr=lr, momentum=0.9)\nbatch_size = 128","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model(classification_model, batch_size, 1000, criterion)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a side note deep learning technique is more suitable for large data, in fact this is why it is widely used and the main purpose it was invented which is to handle learning from large data. So it is understandable why it might not perform perfectly here.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Testing & Submision\nHere I'll test the classification model and save the submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting a batch from testing data\nwith torch.no_grad():\n    for data in test_loader:\n        output = classification_model(data.float())\n        _, preds = torch.max(output.data, 1)\nsurvived = preds.numpy()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived': survived})\nsubmission.to_csv('submission_classification.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"submission_classification.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"References: \n- https://www.kaggle.com/frtgnn/introduction-to-pytorch-a-very-gentle-start\n- https://www.kaggle.com/kiranscaria/titanic-pytorch\n- https://www.kaggle.com/alexisbcook/getting-started-with-titanic\n- https://www.analyticsvidhya.com/blog/2015/12/improve-machine-learning-results/\n- https://androidkt.com/detect-and-remove-outliers-from-pandas-dataframe/","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}